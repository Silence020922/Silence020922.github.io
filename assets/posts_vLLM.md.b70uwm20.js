import{_ as o,o as l,c as s,k as t,a as e,R as a}from"./chunks/framework.Y4B4HMlg.js";const R=JSON.parse('{"title":"vLLM","description":"LLM增添内存管理，利用PagedAttention，提高服务器吞吐量。","frontmatter":{"date":"2024-02-21T00:00:00.000Z","title":"vLLM","tags":["LLM"],"description":"LLM增添内存管理，利用PagedAttention，提高服务器吞吐量。"},"headers":[],"relativePath":"posts/vLLM.md","filePath":"posts/vLLM.md"}'),i={name:"posts/vLLM.md"},n=a('<h2 id="面对问题" tabindex="-1">面对问题 <a class="header-anchor" href="#面对问题" aria-label="Permalink to &quot;面对问题&quot;">​</a></h2><h3 id="现实背景" tabindex="-1">现实背景 <a class="header-anchor" href="#现实背景" aria-label="Permalink to &quot;现实背景&quot;">​</a></h3><ul><li>GPT,Palm这样的大语言模型使得编程助理、聊天机器人等应用成为可能，同时他们也开始进入和影响我们的生活。许多云计算公司竞相提供该类应用程序。然而，目前运行这些程序的成本是昂贵的，需要大量的硬件加速器如GPU，考虑到这些情况，增加吞吐量，降低每次请求的成本便尤为重要。</li><li>GPU计算能力的提升超过了其在内存上的提升，内存越来越成为一显著瓶颈。</li></ul><h3 id="理论背景" tabindex="-1">理论背景 <a class="header-anchor" href="#理论背景" aria-label="Permalink to &quot;理论背景&quot;">​</a></h3>',4),r={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},h={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.98ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -683 1759 705","aria-hidden":"true"},c=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(500,0)" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g></g></g>',1),d=[c],_=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mn",null,"13"),t("mi",null,"B")])],-1),p=t("code",null,"LLM",-1),m=t("br",null,null,-1),g=a('<ul><li>占用内存高。在现有的内存分配策略中，为 KV cache 分配连续的物理内存。由于KV cache 动态的增大或缩小，分配连续的物理内存意味需预先分配一个具有可能最大长度的内存块，这导致了过度留存。由于在整个生命周期中保留了整个块，其他较短的请求不能占用当前块中未使用的任何部分，产生了内部碎片，同时由于每个请求预分配大小的不同，可能产生较大的外部碎片，造成内存浪费。</li><li>由于 KV cache 大小取决于序列长度，具有高度可变和不可预测的特点，因而难以高效管理。</li><li>由于序列的 KV cache 存储在独立的连续空间中，现有的 KV cache 管理方法无法实现内存共享，而很多高级解码算法，例如并行采样使得单个请求产生多个输出，这产生的多个序列部分共享他们的 KV cache 的需求。</li></ul><h2 id="主要成果" tabindex="-1">主要成果 <a class="header-anchor" href="#主要成果" aria-label="Permalink to &quot;主要成果&quot;">​</a></h2><figure class="half"><img src="https://blog.vllm.ai/assets/figures/perf_a100_n1_light.png" width="300"><img src="https://blog.vllm.ai/assets/figures/perf_a10g_n1_light.png" width="300"></figure><figure class="half"><img src="https://blog.vllm.ai/assets/figures/perf_a100_n3_light.png" width="300"><img src="https://blog.vllm.ai/assets/figures/perf_a10g_n3_light.png" width="300"></figure>',4),u=t("li",null,"确定了LLM时内存分配的挑战，并量化了其对LLM性能的影响-Fig.1",-1),T=t("li",null,"受OS中虚拟内存和分页的启发，提出了将 KV cache 存储于非连续分页内存中的方法PagedAttention。",-1),Q=t("li",null,[e("设计"),t("code",null,"vLLM"),e("，一个建立在PagedAttn上的LLM服务引擎。")],-1),x=t("code",null,"vLLM",-1),w={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},f={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.186ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.76ex",height:"1.505ex",role:"img",focusable:"false",viewBox:"0 -583 778 665","aria-hidden":"true"},L=t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mo"},[t("path",{"data-c":"2212",d:"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z",style:{"stroke-width":"3"}})])])],-1),v=[L],b=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mo",null,"−")])],-1),V=t("h2",{id:"pagedattention",tabindex:"-1"},[e("PagedAttention "),t("a",{class:"header-anchor",href:"#pagedattention","aria-label":'Permalink to "PagedAttention"'},"​")],-1),M=t("p",null,[e("以下几个"),t("strong",null,"动图"),e("能够帮助快速理解主要思想。")],-1),k=t("li",null,[e("以不连续的块存储连续的键值，整个思想参考os的虚拟内存，使用连续的逻辑页映射到非连续的物理内存，当用户通过程序访问物理内存时，感知到的如连续分配下相同。具体的，预先将 KV cache 设置为大小固定的块，采用动态分配的策略，在新请下分配一个连续的物理KV块并从左到右依次填充，同时额外维护一个块表，用以记录一个逻辑块对应的物理块的位置和填充的数量，当前请求下直至块满才分配新块。如动图所示，这显然减少了内部资源的浪费(单次请求至多浪费一个块的大小)。 "),t("img",{src:"https://blog.vllm.ai/assets/figures/annimation0.gif",alt:""})],-1),P=t("li",null,[e("块表的翻译 "),t("img",{src:"https://blog.vllm.ai/assets/figures/annimation1.gif",alt:""})],-1),y={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},A={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.09ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.52ex",height:"1.597ex",role:"img",focusable:"false",viewBox:"0 -666 1555.8 706","aria-hidden":"true"},H=a('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1055.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g>',1),S=[H],C=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mo",null,">"),t("mn",null,"1")])],-1),K=t("img",{src:"https://blog.vllm.ai/assets/figures/annimation3.gif",alt:""},null,-1),I=t("li",null,[e("块大小的设置"),t("br"),e(" 这里作为简单的叙述。显然块不能太大，不然序列比块短，则无法共享(每次都要copy-on-write)并且还会产生较大的内部碎片，同时不能太小，太小"),t("code",null,"vLLM"),e("无法充分利用GPU并行性来读取和处理 KV cache。综合考虑，默认16。")],-1);function j(N,Z,q,G,B,D){return l(),s("div",null,[n,t("p",null,[e("在NVIDIA A100使用 "),t("mjx-container",r,[(l(),s("svg",h,d)),_]),e(" 参数的"),p,e("时内存布局现实，大约65%内存分配给模型权重，该部分在服务期间保持静态，临时激活部分仅占用GPU内存的一小部分，故而管理KV cache 对服务器处理请求的能力至关重要。"),m,e(" KV(key value) cache原本作为牺牲空间换取运算速度的策略，但其具有一些问题：")]),g,t("ul",null,[u,T,Q,t("li",null,[e("评估了各种情况下"),x,e("的表现，并证实其确实大大优于先前最先进的解决方案-Fig.12"),t("mjx-container",w,[(l(),s("svg",f,v)),b]),e("Fig.18")])]),V,M,t("ul",null,[k,P,t("li",null,[e("内存共享方面，以并行取样为例。在并行采样中，一个请求产生多个序列，从而多个序列中包含有多个相同的提示文本，图示为两个输出并行解码的示例，由于两个输出共享一个提示符，在最初阶段只需要为该提示符提供一个副本空间，两个序列的逻辑块被映射到相同的物理块中，由于每个物理块可被多次映射，故增添一个引用计数器，当识别到该块引用计数"),t("mjx-container",y,[(l(),s("svg",A,S)),C]),e("，则执行写时复制操作，分配新的物理块并复制原物理块的信息，并将引用计数减少1,如动图所示。 "),K]),I])])}const J=o(i,[["render",j]]);export{R as __pageData,J as default};
