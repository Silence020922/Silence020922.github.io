import{_ as o,o as l,c as s,k as t,a as e,R as a}from"./chunks/framework.CP3Xps-Z.js";const R=JSON.parse('{"title":"vLLM","description":"LLM增添内存管理，利用PagedAttention，提高服务器吞吐量。","frontmatter":{"date":"2024-02-21T00:00:00.000Z","title":"vLLM","tags":["LLM"],"description":"LLM增添内存管理，利用PagedAttention，提高服务器吞吐量。"},"headers":[],"relativePath":"posts/vLLM.md","filePath":"posts/vLLM.md"}'),i={name:"posts/vLLM.md"},n=a("",4),r={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},h={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.98ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -683 1759 705","aria-hidden":"true"},c=a("",1),d=[c],_=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mn",null,"13"),t("mi",null,"B")])],-1),p=t("code",null,"LLM",-1),m=t("br",null,null,-1),g=a("",4),u=t("li",null,"确定了LLM时内存分配的挑战，并量化了其对LLM性能的影响-Fig.1",-1),T=t("li",null,"受OS中虚拟内存和分页的启发，提出了将 KV cache 存储于非连续分页内存中的方法PagedAttention。",-1),Q=t("li",null,[e("设计"),t("code",null,"vLLM"),e("，一个建立在PagedAttn上的LLM服务引擎。")],-1),x=t("code",null,"vLLM",-1),w={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},f={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.186ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.76ex",height:"1.505ex",role:"img",focusable:"false",viewBox:"0 -583 778 665","aria-hidden":"true"},L=t("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[t("g",{"data-mml-node":"math"},[t("g",{"data-mml-node":"mo"},[t("path",{"data-c":"2212",d:"M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z",style:{"stroke-width":"3"}})])])],-1),v=[L],b=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mo",null,"−")])],-1),V=t("h2",{id:"pagedattention",tabindex:"-1"},[e("PagedAttention "),t("a",{class:"header-anchor",href:"#pagedattention","aria-label":'Permalink to "PagedAttention"'},"​")],-1),M=t("p",null,[e("以下几个"),t("strong",null,"动图"),e("能够帮助快速理解主要思想。")],-1),k=t("li",null,[e("以不连续的块存储连续的键值，整个思想参考os的虚拟内存，使用连续的逻辑页映射到非连续的物理内存，当用户通过程序访问物理内存时，感知到的如连续分配下相同。具体的，预先将 KV cache 设置为大小固定的块，采用动态分配的策略，在新请下分配一个连续的物理KV块并从左到右依次填充，同时额外维护一个块表，用以记录一个逻辑块对应的物理块的位置和填充的数量，当前请求下直至块满才分配新块。如动图所示，这显然减少了内部资源的浪费(单次请求至多浪费一个块的大小)。 "),t("img",{src:"https://blog.vllm.ai/assets/figures/annimation0.gif",alt:""})],-1),P=t("li",null,[e("块表的翻译 "),t("img",{src:"https://blog.vllm.ai/assets/figures/annimation1.gif",alt:""})],-1),y={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},A={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.09ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.52ex",height:"1.597ex",role:"img",focusable:"false",viewBox:"0 -666 1555.8 706","aria-hidden":"true"},H=a("",1),S=[H],C=t("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[t("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[t("mo",null,">"),t("mn",null,"1")])],-1),K=t("img",{src:"https://blog.vllm.ai/assets/figures/annimation3.gif",alt:""},null,-1),I=t("li",null,[e("块大小的设置"),t("br"),e(" 这里作为简单的叙述。显然块不能太大，不然序列比块短，则无法共享(每次都要copy-on-write)并且还会产生较大的内部碎片，同时不能太小，太小"),t("code",null,"vLLM"),e("无法充分利用GPU并行性来读取和处理 KV cache。综合考虑，默认16。")],-1);function j(N,Z,q,G,B,D){return l(),s("div",null,[n,t("p",null,[e("在NVIDIA A100使用 "),t("mjx-container",r,[(l(),s("svg",h,d)),_]),e(" 参数的"),p,e("时内存布局现实，大约65%内存分配给模型权重，该部分在服务期间保持静态，临时激活部分仅占用GPU内存的一小部分，故而管理KV cache 对服务器处理请求的能力至关重要。"),m,e(" KV(key value) cache原本作为牺牲空间换取运算速度的策略，但其具有一些问题：")]),g,t("ul",null,[u,T,Q,t("li",null,[e("评估了各种情况下"),x,e("的表现，并证实其确实大大优于先前最先进的解决方案-Fig.12"),t("mjx-container",w,[(l(),s("svg",f,v)),b]),e("Fig.18")])]),V,M,t("ul",null,[k,P,t("li",null,[e("内存共享方面，以并行取样为例。在并行采样中，一个请求产生多个序列，从而多个序列中包含有多个相同的提示文本，图示为两个输出并行解码的示例，由于两个输出共享一个提示符，在最初阶段只需要为该提示符提供一个副本空间，两个序列的逻辑块被映射到相同的物理块中，由于每个物理块可被多次映射，故增添一个引用计数器，当识别到该块引用计数"),t("mjx-container",y,[(l(),s("svg",A,S)),C]),e("，则执行写时复制操作，分配新的物理块并复制原物理块的信息，并将引用计数减少1,如动图所示。 "),K]),I])])}const J=o(i,[["render",j]]);export{R as __pageData,J as default};
