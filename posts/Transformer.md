---
date: 2024-02-27
title: Transformer
tags:
- LLM
description:  参考论文 Attention is all you need
---
## 补充知识
- 矩阵乘法复杂度:
参考[CSDN](https://blog.csdn.net/qq_39463175/article/details/111818717)
- Adam 优化器:
参考[知乎](https://zhuanlan.zhihu.com/p/32698042)
- BLEU score:
参考[知乎](https://zhuanlan.zhihu.com/p/350596071)
- Label smoothing:
参考[When does label smoothing help?](https://arxiv.org/abs/1906.02629)
- 束搜索:
参考[Bilibili](https://www.bilibili.com/video/BV1B44y1C7m1/?spm_id_from=333.1007.top_right_bar_window_history.content.click)
- PPL (perplexity):
参考[知乎](https://zhuanlan.zhihu.com/p/350804577)
## 模型解析
参考[知乎](https://zhuanlan.zhihu.com/p/48508221) 或 [笔记](https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer) 或 [Jay Alammer blog](https://jalammar.github.io/illustrated-transformer/)
## 代码实现
[pytorch](https://nlp.seas.harvard.edu/2018/04/03/attention.html) 或 [tensorflow](https://github.com/tensorflow/tensor2tensor)
