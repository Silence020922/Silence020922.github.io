<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>vLLM | Silence's blog</title>
    <meta name="description" content="LLM增添内存管理，利用PagedAttention，提高服务器吞吐量。">
    <meta name="generator" content="VitePress v1.0.0-rc.31">
    <link rel="preload stylesheet" href="/assets/style.jx_1Qzz9.css" as="style">
    
    <script type="module" src="/assets/app.53cN0pXm.js"></script>
    <link rel="preload" href="/assets/inter-roman-latin.bvIUbFQP.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/assets/chunks/framework.CP3Xps-Z.js">
    <link rel="modulepreload" href="/assets/chunks/theme.NB9nl4GV.js">
    <link rel="modulepreload" href="/assets/chunks/Page.qgDUVnsQ.js">
    <link rel="modulepreload" href="/assets/posts_vLLM.md.l3wB_VjQ.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"dark",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><!--[--><div class="Layout" data-v-9d8abc1e><!--[--><!--]--><!--[--><span tabindex="-1" data-v-c8291ffa></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-c8291ffa> Skip to content </a><!--]--><!----><header class="VPNav" data-v-9d8abc1e data-v-7ad780c2><div class="VPNavBar" data-v-7ad780c2 data-v-5befd255><div class="container" data-v-5befd255><div class="title" data-v-5befd255><div class="VPNavBarTitle" data-v-5befd255 data-v-2973dbb4><a class="title" href="/" data-v-2973dbb4><!--[--><!--]--><!----><!--[-->Silence&#39;s blog<!--]--><!--[--><!--]--></a></div></div><div class="content" data-v-5befd255><div class="curtain" data-v-5befd255></div><div class="content-body" data-v-5befd255><!--[--><!--]--><div class="VPNavBarSearch search" data-v-5befd255><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg class="DocSearch-Search-Icon" width="20" height="20" viewBox="0 0 20 20" aria-label="search icon"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-5befd255 data-v-f732b5d0><span id="main-nav-aria-label" class="visually-hidden" data-v-f732b5d0>Main Navigation</span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Home</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/pages/archives.html" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Archives</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/pages/tags.html" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>Tags</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/pages/about.html" tabindex="0" data-v-f732b5d0 data-v-cb318fec><!--[--><span data-v-cb318fec>About</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-5befd255 data-v-283b26e9><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to light theme" aria-checked="true" data-v-283b26e9 data-v-70af5d02 data-v-1c29e291><span class="check" data-v-1c29e291><span class="icon" data-v-1c29e291><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-70af5d02><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-70af5d02><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><!----><div class="VPFlyout VPNavBarExtra extra" data-v-5befd255 data-v-8e87c032 data-v-aa8de344><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-aa8de344><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="icon" data-v-aa8de344><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg></button><div class="menu" data-v-aa8de344><div class="VPMenu" data-v-aa8de344 data-v-e42ed9b3><!----><!--[--><!--[--><!----><div class="group" data-v-8e87c032><div class="item appearance" data-v-8e87c032><p class="label" data-v-8e87c032>Appearance</p><div class="appearance-action" data-v-8e87c032><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title="Switch to light theme" aria-checked="true" data-v-8e87c032 data-v-70af5d02 data-v-1c29e291><span class="check" data-v-1c29e291><span class="icon" data-v-1c29e291><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="sun" data-v-70af5d02><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="moon" data-v-70af5d02><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><!----><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-5befd255 data-v-6bee1efd><span class="container" data-v-6bee1efd><span class="top" data-v-6bee1efd></span><span class="middle" data-v-6bee1efd></span><span class="bottom" data-v-6bee1efd></span></span></button></div></div></div></div><!----></header><div class="VPLocalNav fixed reached-top" data-v-9d8abc1e data-v-f8a0b38a><!----><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-f8a0b38a data-v-24251f6f><button data-v-24251f6f>Return to top</button><!----></div></div><!----><div class="VPContent" id="VPContent" data-v-9d8abc1e data-v-3cf691b6><div class="VPDoc has-aside" data-v-3cf691b6 data-v-a3c25e27><!--[--><!--]--><div class="container" data-v-a3c25e27><div class="aside" data-v-a3c25e27><div class="aside-curtain" data-v-a3c25e27></div><div class="aside-container" data-v-a3c25e27><div class="aside-content" data-v-a3c25e27><div class="VPDocAside" data-v-a3c25e27 data-v-cb998dce><!--[--><!--]--><!--[--><!--]--><div class="VPDocAsideOutline" role="navigation" data-v-cb998dce data-v-3a6c4994><div class="content" data-v-3a6c4994><div class="outline-marker" data-v-3a6c4994></div><div class="outline-title" role="heading" aria-level="2" data-v-3a6c4994>文章摘要</div><nav aria-labelledby="doc-outline-aria-label" data-v-3a6c4994><span class="visually-hidden" id="doc-outline-aria-label" data-v-3a6c4994> Table of Contents for current page </span><ul class="root" data-v-3a6c4994 data-v-463da30f><!--[--><!--]--></ul></nav></div></div><!--[--><!--]--><div class="spacer" data-v-cb998dce></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-a3c25e27><div class="content-container" data-v-a3c25e27><!--[--><!--]--><!----><main class="main" data-v-a3c25e27><div style="position:relative;" class="vp-doc _posts_vLLM" data-v-a3c25e27><div><h2 id="面对问题" tabindex="-1">面对问题 <a class="header-anchor" href="#面对问题" aria-label="Permalink to &quot;面对问题&quot;">​</a></h2><h3 id="现实背景" tabindex="-1">现实背景 <a class="header-anchor" href="#现实背景" aria-label="Permalink to &quot;现实背景&quot;">​</a></h3><ul><li>GPT,Palm这样的大语言模型使得编程助理、聊天机器人等应用成为可能，同时他们也开始进入和影响我们的生活。许多云计算公司竞相提供该类应用程序。然而，目前运行这些程序的成本是昂贵的，需要大量的硬件加速器如GPU，考虑到这些情况，增加吞吐量，降低每次请求的成本便尤为重要。</li><li>GPU计算能力的提升超过了其在内存上的提升，内存越来越成为一显著瓶颈。</li></ul><h3 id="理论背景" tabindex="-1">理论背景 <a class="header-anchor" href="#理论背景" aria-label="Permalink to &quot;理论背景&quot;">​</a></h3><p>在NVIDIA A100使用 <mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.05ex;" xmlns="http://www.w3.org/2000/svg" width="3.98ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1759 705" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z" transform="translate(500,0)" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(1000,0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>13</mn><mi>B</mi></math></mjx-assistive-mml></mjx-container> 参数的<code>LLM</code>时内存布局现实，大约65%内存分配给模型权重，该部分在服务期间保持静态，临时激活部分仅占用GPU内存的一小部分，故而管理KV cache 对服务器处理请求的能力至关重要。<br> KV(key value) cache原本作为牺牲空间换取运算速度的策略，但其具有一些问题：</p><ul><li>占用内存高。在现有的内存分配策略中，为 KV cache 分配连续的物理内存。由于KV cache 动态的增大或缩小，分配连续的物理内存意味需预先分配一个具有可能最大长度的内存块，这导致了过度留存。由于在整个生命周期中保留了整个块，其他较短的请求不能占用当前块中未使用的任何部分，产生了内部碎片，同时由于每个请求预分配大小的不同，可能产生较大的外部碎片，造成内存浪费。</li><li>由于 KV cache 大小取决于序列长度，具有高度可变和不可预测的特点，因而难以高效管理。</li><li>由于序列的 KV cache 存储在独立的连续空间中，现有的 KV cache 管理方法无法实现内存共享，而很多高级解码算法，例如并行采样使得单个请求产生多个输出，这产生的多个序列部分共享他们的 KV cache 的需求。</li></ul><h2 id="主要成果" tabindex="-1">主要成果 <a class="header-anchor" href="#主要成果" aria-label="Permalink to &quot;主要成果&quot;">​</a></h2><figure class="half"><img src="https://blog.vllm.ai/assets/figures/perf_a100_n1_light.png" width="300"><img src="https://blog.vllm.ai/assets/figures/perf_a10g_n1_light.png" width="300"></figure><figure class="half"><img src="https://blog.vllm.ai/assets/figures/perf_a100_n3_light.png" width="300"><img src="https://blog.vllm.ai/assets/figures/perf_a10g_n3_light.png" width="300"></figure><ul><li>确定了LLM时内存分配的挑战，并量化了其对LLM性能的影响-Fig.1</li><li>受OS中虚拟内存和分页的启发，提出了将 KV cache 存储于非连续分页内存中的方法PagedAttention。</li><li>设计<code>vLLM</code>，一个建立在PagedAttn上的LLM服务引擎。</li><li>评估了各种情况下<code>vLLM</code>的表现，并证实其确实大大优于先前最先进的解决方案-Fig.12<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.186ex;" xmlns="http://www.w3.org/2000/svg" width="1.76ex" height="1.505ex" role="img" focusable="false" viewBox="0 -583 778 665" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>−</mo></math></mjx-assistive-mml></mjx-container>Fig.18</li></ul><h2 id="pagedattention" tabindex="-1">PagedAttention <a class="header-anchor" href="#pagedattention" aria-label="Permalink to &quot;PagedAttention&quot;">​</a></h2><p>以下几个<strong>动图</strong>能够帮助快速理解主要思想。</p><ul><li>以不连续的块存储连续的键值，整个思想参考os的虚拟内存，使用连续的逻辑页映射到非连续的物理内存，当用户通过程序访问物理内存时，感知到的如连续分配下相同。具体的，预先将 KV cache 设置为大小固定的块，采用动态分配的策略，在新请下分配一个连续的物理KV块并从左到右依次填充，同时额外维护一个块表，用以记录一个逻辑块对应的物理块的位置和填充的数量，当前请求下直至块满才分配新块。如动图所示，这显然减少了内部资源的浪费(单次请求至多浪费一个块的大小)。 <img src="https://blog.vllm.ai/assets/figures/annimation0.gif" alt=""></li><li>块表的翻译 <img src="https://blog.vllm.ai/assets/figures/annimation1.gif" alt=""></li><li>内存共享方面，以并行取样为例。在并行采样中，一个请求产生多个序列，从而多个序列中包含有多个相同的提示文本，图示为两个输出并行解码的示例，由于两个输出共享一个提示符，在最初阶段只需要为该提示符提供一个副本空间，两个序列的逻辑块被映射到相同的物理块中，由于每个物理块可被多次映射，故增添一个引用计数器，当识别到该块引用计数<mjx-container class="MathJax" jax="SVG" style="direction:ltr;position:relative;"><svg style="overflow:visible;min-height:1px;min-width:1px;vertical-align:-0.09ex;" xmlns="http://www.w3.org/2000/svg" width="3.52ex" height="1.597ex" role="img" focusable="false" viewBox="0 -666 1555.8 706" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1055.8,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline" style="top:0px;left:0px;clip:rect(1px, 1px, 1px, 1px);-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;position:absolute;padding:1px 0px 0px 0px;border:0px;display:block;width:auto;overflow:hidden;"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo>&gt;</mo><mn>1</mn></math></mjx-assistive-mml></mjx-container>，则执行写时复制操作，分配新的物理块并复制原物理块的信息，并将引用计数减少1,如动图所示。 <img src="https://blog.vllm.ai/assets/figures/annimation3.gif" alt=""></li><li>块大小的设置<br> 这里作为简单的叙述。显然块不能太大，不然序列比块短，则无法共享(每次都要copy-on-write)并且还会产生较大的内部碎片，同时不能太小，太小<code>vLLM</code>无法充分利用GPU并行性来读取和处理 KV cache。综合考虑，默认16。</li></ul></div></div></main><footer class="VPDocFooter" data-v-a3c25e27 data-v-b4b63abf><!--[--><!--]--><!----><!----></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div><div class="site-footer"> MIT Licensed | Copyright © 2021-2022 <a class="vitepress" href="https://github.com/Silence020922">Silence&#39;s blog</a><br> Powered by <a class="vitepress" target="_blank" href="//vitepress.vuejs.org/">VitePress - 1.0.0-rc.25</a> Theme by <a class="vitepress" target="_blank" href="//github.com/airene/vitepress-blog-pure">Vitepress-blog</a></div><!--]--></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"pages_tags.md\":\"_H06iygS\",\"pages_archives.md\":\"Y4x88mh_\",\"page_2.md\":\"hg-DmTYW\",\"index.md\":\"KdBfAfwO\",\"page_3.md\":\"IXVU8ZT_\",\"posts_cuda.md\":\"puoekQ9W\",\"pages_about.md\":\"zm3AryHG\",\"posts_flashattn.md\":\"uZGWk_Tb\",\"posts_vitepress-first.md\":\"a_Ol6IXs\",\"posts_transformer.md\":\"VJ7DBgZ_\",\"posts_vllm.md\":\"l3wB_VjQ\",\"posts_gnn-gcn.md\":\"RaQ93kzt\",\"posts_gnn-agcn.md\":\"Vz13XZAC\",\"posts_theme-r.md\":\"hHO1s_cX\",\"posts_gnn-introduction.md\":\"jRIVIQSk\",\"posts_gnn-rayleigh.md\":\"Bnj_EVuk\",\"posts_gnn-acmp.md\":\"hd-s1wPg\",\"posts_gnn-gcnii.md\":\"cOp8Foeh\",\"posts_gnn-chebyshev.md\":\"pZNoeOVL\",\"posts_pag.md\":\"lUg7yX_N\",\"posts_combinatorial_optimizition.md\":\"NUFb2FHH\",\"posts_gc-gceffects.md\":\"VbGvxQ5L\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Silence's blog\",\"description\":\"Silence's blog\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":\"dark\",\"themeConfig\":{\"posts\":[{\"frontMatter\":{\"date\":\"2024-03-01\",\"title\":\"CUDA编程\",\"tags\":[\"CUDA\"],\"description\":\"无...\"},\"regularPath\":\"/posts/CUDA.html\"},{\"frontMatter\":{\"date\":\"2024-02-27\",\"title\":\"Transformer\",\"tags\":[\"LLM\"],\"description\":\"参考论文 Attention is all you need\"},\"regularPath\":\"/posts/Transformer.html\"},{\"frontMatter\":{\"date\":\"2024-02-21\",\"title\":\"vLLM\",\"tags\":[\"LLM\"],\"description\":\"LLM增添内存管理，利用PagedAttention，提高服务器吞吐量。\"},\"regularPath\":\"/posts/vLLM.html\"},{\"frontMatter\":{\"date\":\"2024-02-21\",\"title\":\"FlashAttention\",\"tags\":[\"LLM\"],\"description\":\"加速transormer模型训练速度，以便应用到较长上下文中。\"},\"regularPath\":\"/posts/FlashAttn.html\"},{\"frontMatter\":{\"date\":\"2024-01-21\",\"title\":\"Preferential Attachment Graph\",\"tags\":[\"Random grpah\",\"Preferential Attachment Graph\"],\"description\":\"反应现实世界幂律分布现象的一种随机图的定义及性质。\"},\"regularPath\":\"/posts/PAG.html\"},{\"frontMatter\":{\"date\":\"2024-01-08\",\"title\":\"Combinatorial Optimizition\",\"tags\":[\"Combinatorial Optimizition notes\"],\"description\":\"组合优化课程笔记\"},\"regularPath\":\"/posts/Combinatorial_Optimizition.html\"},{\"frontMatter\":{\"date\":\"2024-01-04\",\"title\":\"Effects of GC\",\"tags\":[\"GNN\"],\"description\":\"8-th，自GNN面世后，衍生了很多的变式，也存在很多实际的应用。但对向神经网络中添加卷积算子的意义本身，却缺乏理论上的证明，本文将携带卷积算子的神经网络与不使用图信息的神经网络进行对比，从理论出发给出严密的证明。参考Effects of graph convolutions in multi-layer networks\"},\"regularPath\":\"/posts/GC-GCEffects.html\"},{\"frontMatter\":{\"date\":\"2023-12-10\",\"title\":\"ACMP\",\"tags\":[\"GNN\"],\"description\":\"7-th，本文从神经信息传递的角度推到出模型并从能量函数的角度给出理论证明。参考Acmp:Allen-cahn message passing with attractive and repulsive forces for graph neural networks\"},\"regularPath\":\"/posts/GNN-ACMP.html\"},{\"frontMatter\":{\"date\":\"2023-12-09\",\"title\":\"GCNII\",\"tags\":[\"GNN\"],\"description\":\"6-th,参考SEMI-surperised classification with graph convolutional networks\"},\"regularPath\":\"/posts/GNN-GCNII.html\"},{\"frontMatter\":{\"date\":\"2023-12-05\",\"title\":\"AGCN\",\"tags\":[\"GNN\"],\"description\":\"5-th,考虑到在GCN模型中，由于表达与邻接矩阵直接相关，邻居节点的重要性由中心节点一圈一圈向外散布，这限制了卷积核的flexibility。这里考虑通过学习广义马氏距离替代邻接矩阵，实现对拉普拉斯矩阵的参数化设计，增加模型的表达能力。参考Adaptive Graph Convolutional Neural Networks。\"},\"regularPath\":\"/posts/GNN-AGCN.html\"},{\"frontMatter\":{\"date\":\"2023-12-04\",\"title\":\"GCN\",\"tags\":[\"GNN\"],\"description\":\"4-th,参考SEMI-surperised classification with graph convolutional networks\"},\"regularPath\":\"/posts/GNN-GCN.html\"},{\"frontMatter\":{\"date\":\"2023-12-03\",\"title\":\"Eigenvalues of the Laplacian\",\"tags\":[\"Rayleigh quotient\",\"Laplacian matrix\"],\"description\":\"3-rd,给出拉普拉斯矩阵特征值范围[0,2]的证明。\"},\"regularPath\":\"/posts/GNN-Rayleigh.html\"},{\"frontMatter\":{\"date\":\"2023-11-30\",\"title\":\"ChebNet\",\"tags\":[\"GNN\",\"Chebyshev\"],\"description\":\"2-nd,参考Convolution Neural Networks on Graphs with Fast Localized Spectral Filtering\"},\"regularPath\":\"/posts/GNN-Chebyshev.html\"},{\"frontMatter\":{\"date\":\"2023-11-29\",\"title\":\"Basic graph convolution operator\",\"tags\":[\"GNN\",\"GSP\"],\"description\":\"1-st，参考《深入浅出图神经网络》。\"},\"regularPath\":\"/posts/GNN-introduction.html\"},{\"frontMatter\":{\"title\":\"R语言入门\",\"date\":\"2022-10-17\",\"tags\":[\"R\"],\"description\":\"专业课——统计软件的随笔，这里使用的参考书目是《R语言实战(第二版)》\"},\"regularPath\":\"/posts/theme-R.html\"},{\"frontMatter\":{\"date\":\"2021-06-30\",\"title\":\"一直想找一个系统架构和设计都足够干净的系统\",\"tags\":[\"vitepress\",\"markdown\"],\"description\":\"vitepress的markdown插件支持的语法，一直想找一个干净的系统架构和设计都足够干净都，一直没满意的，不满意就自己设计，一直想找一个干净的系统架构和设计都足够干净都，一直没满意的，不满意就自己设计\"},\"regularPath\":\"/posts/vitepress-first.html\"}],\"website\":\"https://github.com/Silence020922\",\"nav\":[{\"text\":\"Home\",\"link\":\"/\"},{\"text\":\"Archives\",\"link\":\"/pages/archives\"},{\"text\":\"Tags\",\"link\":\"/pages/tags\"},{\"text\":\"About\",\"link\":\"/pages/about\"}],\"search\":{\"provider\":\"local\"},\"outlineTitle\":\"文章摘要\"},\"locales\":{},\"scrollOffset\":90,\"cleanUrls\":false}");</script>
    
  </body>
</html>